{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yoshiho0/AI-for-transportation/blob/main/exercise_5_clustering_checkpoint.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RAlPf9WW9D9O"
      },
      "source": [
        "# Revealing representative \"typical\" day-types using traffic data observation and clustering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Kvwq8JAXHGW"
      },
      "source": [
        "Clustering is a way of letting the data tell us its own story. Instead of assuming traffic always follows the same daily rhythm, we will use clustering to discover “typical day-types”: weekdays, weekends, holidays, incidents, straight from sensor observations. By the end of this exercise, you will learn how to prepare temporal data for clustering, explore it and reflect on what the patterns might mean for managing roads in practice.\n",
        "\n",
        "Objective: find, evaluate and choose a clustering of daily time-series (day-profiles), and show how that clustering helps a simple prediction task.\n",
        "\n",
        "what this notebook does:\n",
        "\n",
        "Data loading & Exploratory Data Analysis (visualize raw day profiles).\n",
        "Preprocessing\n",
        "Baseline experiments (k-means), time-series alternatives (k-Shape or DTW), and at least one density/soft model (GMM/DBSCAN).\n",
        "Internal evaluation (silhouette, Calinski-Harabasz, Davies-Bouldin) + stability checks.\n",
        "Time-aware external evaluation using the centroid-based historical-mean predictor (RMSE, MAPE).\n",
        "Interpretation: centroids, representative days and weekday distribution\n",
        "Glossary\n",
        "day-profile / day-matrix: one row = one day; columns = 5-minute buckets.\n",
        "centroid historical-mean predictor: for each cluster, predict a day by using the cluster centroid (or average of historical days assigned to that cluster)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1IWUHGQ-quPi"
      },
      "source": [
        "## Data preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YIbPBmGqxh00"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "#upload the dataset by downloading both datasets from canvas and upload it on colab\n",
        "\n",
        "data_df = pd.read_csv(\"dataset_exercise_5_clustering_highway_traffic.csv\",sep=\";\")\n",
        "data_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGCheycS96cI"
      },
      "source": [
        "The provided dataset contains 5-minute observations on one highway microwave sensor and needs to be in a format ready for clustering. Clustering doesn’t work on raw tables, it needs a structured representation. In this section, we will take the raw sensor data and reformat it so that each entire day becomes a single data vector. This makes it possible to compare whole days against each other.\n",
        "\n",
        "Here’s the logic: a day is 24 hours long, and with 5-minute measurements that means 288 data points. If we line these up in order, we can represent each day as a 288-dimensional vector: one row per day, one column per time slot. Later, clustering will group these day-vectors into families of “similar days.” This is \"giving every day its own fingerprint.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2IYSKJGoy4T4"
      },
      "outputs": [],
      "source": [
        "# Sort the DataFrame 'data_df' by columns \"Date\" and \"Interval_5\"\n",
        "data_df.sort_values([\"Date\", \"Interval_5\"])\n",
        "\n",
        "# Extract unique dates from the sorted DataFrame\n",
        "days = np.unique(data_df[['Date']].values.ravel())\n",
        "# Calculate the total number of unique days\n",
        "ndays = len(days)\n",
        "\n",
        "# Group the DataFrame 'data_df' by the \"Date\" column\n",
        "day_subsets_df = data_df.groupby([\"Date\"])\n",
        "\n",
        "# Define the total number of 5-minute intervals in a day\n",
        "nintvals = 288\n",
        "\n",
        "# Create a matrix 'vectorized_day_dataset' filled with NaN values\n",
        "vectorized_day_dataset = np.zeros((ndays, nintvals))\n",
        "vectorized_day_dataset.fill(np.nan)\n",
        "\n",
        "# Loop through each unique day\n",
        "for i in range(0, ndays):\n",
        "    # Get the DataFrame corresponding to the current day\n",
        "    df_t = day_subsets_df.get_group(days[i])\n",
        "\n",
        "    # Loop through each row in the current day's DataFrame\n",
        "    for j in range(len(df_t)):\n",
        "        # Get the current day's DataFrame\n",
        "        df_t = day_subsets_df.get_group(days[i])\n",
        "\n",
        "        # Extract the \"Interval_5\" and \"flow\" values and populate 'vectorized_day_dataset'\n",
        "        vectorized_day_dataset[i, df_t.iloc[j][\"Interval_5\"]] = df_t.iloc[j][\"flow\"]\n",
        "\n",
        "# Print the resulting 'vectorized_day_dataset'\n",
        "print(vectorized_day_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6w2V4_X_Jte"
      },
      "source": [
        "Recap of the steps the code did:\n",
        "\n",
        "STEP 1:We sorted the data to guarantee chronological order.\n",
        "STEP 2:We pulled out all unique days, so we know how many daily profiles we need.\n",
        "STEP 3:We fixed the number of time slots (288) so every day has the same length.\n",
        "STEP 4:We built a big empty matrix, one row per day, one column per 5-minute slot.\n",
        "STEP 5:Finally, we filled this matrix: for each day, we inserted the flow values into the right time slots.\n",
        "The result: a clean “day × time” dataset ready for clustering. From here on, each row can be treated like a single point in a 288-dimensional space.\n",
        "\n",
        "Note that if you plan to reveal network-wide day-types (more sensors), this day vector can be ordered vector of SENSORS * TIME-INTERVALS."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4xan1q3Ai-v"
      },
      "source": [
        "# Part 1: Data exploration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5o7Jq8sXHGY"
      },
      "source": [
        "# 1.1 : Missing values and outliers\n",
        "\n",
        "Before we trust this matrix, we need to explore it. Clustering algorithms are very sensitive to missing values and outliers: a single \"bad day\" can distort entire clusters. Let’s check for gaps and anomalies before we hand the data to the clustering algorithm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KCycuUGJXajK"
      },
      "outputs": [],
      "source": [
        "\n",
        "print('number of nans',np.sum(np.isnan(vectorized_day_dataset)))\n",
        "print('rate of nans',np.sum(np.isnan(vectorized_day_dataset))/(ndays*nintvals))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BiCP8S1bAyhS"
      },
      "source": [
        "The quick check shows only 277 missing entries, which is just 0.26% of the data. That is very low but missingness is rarely random. In traffic data, it could mean times when no vehicles were detected (night hours) or moments when sensors temporarily failed. Spotting where/when these gaps happen helps us decide whether to ignore them, fill them in or treat them as a signal themselves."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iz1oAfKR_LM0"
      },
      "outputs": [],
      "source": [
        "nans_per_time = np.sum(np.isnan(vectorized_day_dataset),0)\n",
        "print(nans_per_time.shape)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "# Create an array 'x_axis' representing the 5-minute intervals\n",
        "x_axis = np.arange(0, nintvals, 1, dtype=int)\n",
        "# Initialize an empty list 'x_axis_hours' to store time values in hours\n",
        "x_axis_hours = []\n",
        "# Convert interval indices to hours and append them to 'x_axis_hours'\n",
        "for i in range(0, len(x_axis)):\n",
        "  x_axis_hours.append(float(x_axis[i]*5)/60)\n",
        "ax.bar(x_axis_hours,height=nans_per_time)\n",
        "\n",
        "\n",
        "ax.set_ylabel('number of missing values')\n",
        "ax.set_xlabel('5-minute intervals')\n",
        "ax.set_title('Time profile of missing values')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZjUIBmjBaWt"
      },
      "source": [
        "Are these missing values associated with just a few days?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TppVTgUhDvbi"
      },
      "outputs": [],
      "source": [
        "nans_per_day = np.sum(np.isnan(vectorized_day_dataset),1)\n",
        "print('number of days with missing value',np.size(np.where(nans_per_day > 0),1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vChxVpEUXHGY"
      },
      "source": [
        "# Part 1.2 : Daily profile of flow dynamic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jByZuZZfGCqN"
      },
      "source": [
        "Visualizing traffic flow across time of day reveals the heartbeat of the system. What does the data look like? What is the traffic pattern? You should expect sharp rises around commuting hours and dips overnight. These daily profiles are important because clustering algorithms will later decide “similarity” based on the shapes of these curves. When you see peaks or irregular bumps, think about what real world behaviors they correspond to: work commute, evening leisure, freight transport.\n",
        "\n",
        "Below is the script that can help you visualize all days where overlapping transparencies highlight some patterns. The black line is the average yearly flow for a time interval."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d2CEwLhwGEln"
      },
      "outputs": [],
      "source": [
        "# Create a new figure and axis object using subplots\n",
        "fig, ax = plt.subplots()# a convenient way to create a new figure and a set of subplots.\n",
        "ax.plot(np.array([x_axis_hours,]*ndays).transpose(),vectorized_day_dataset.transpose(),color='#444444',alpha=0.05)\n",
        "# Above line plots the dataset with specified color and transparency.\n",
        "ax.plot(x_axis_hours,np.transpose(np.nanmean(vectorized_day_dataset,0)),color='black')\n",
        "# Above line plots the average of the dataset in black color.\n",
        "\n",
        "ax.set_ylabel('Average flow')\n",
        "ax.set_xlabel('5-minute intervals')\n",
        "plt.xlim(0,24)\n",
        "ax.set_title('Daily profile of flow dynamic')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rGecQQ6YXHGY"
      },
      "source": [
        "Boxplots compress the daily data into distributions for each 5-minute slot. They tell us not only the average but also the spread and the outliers. Below is the code to visualize these boxplots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j-7BQUzQcacL"
      },
      "outputs": [],
      "source": [
        "# Create a new figure and axis object using subplots\n",
        "fig, ax = plt.subplots()  # This line is a convenient way to create a new figure and a set of subplots.\n",
        "\n",
        "# Create a boxplot for the dataset\n",
        "boxplot = ax.boxplot(vectorized_day_dataset.T, patch_artist=True)\n",
        "\n",
        "# Customize the boxplot appearance\n",
        "for patch in boxplot['boxes']:\n",
        "    patch.set_facecolor('#444444')  # Set the box color to gray\n",
        "for median in boxplot['medians']:\n",
        "    median.set(color='black', linewidth=2)  # Set median line color to black\n",
        "\n",
        "# Set the y-axis label\n",
        "ax.set_ylabel('Flow')\n",
        "\n",
        "# Set the x-axis label\n",
        "ax.set_xlabel('5-minute intervals')\n",
        "\n",
        "# Set the x-axis limits to be between 0 and 24\n",
        "plt.xlim(0, 24)\n",
        "\n",
        "# Set the title of the plot\n",
        "ax.set_title('Daily Profile of Flow Dynamics (Boxplot)')\n",
        "\n",
        "# Display the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQvI-q1PXHGY"
      },
      "source": [
        "Notice whether morning peaks are tight (predictable) or wide (variable). Wide distributions could mean more “noise” for clustering, the algorithm will find it harder to decide what is typical for that time slot."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XECofu54LbHT"
      },
      "source": [
        "By averaging days into weekdays vs. weekends, we test a very human hypothesis: “Mondays don’t look like Sundays.” Do the curves confirm it? This step bridges human intuition (calendar categories) with machine learning exploration (clustering). Later, we’ll see whether the algorithm rediscovers these patterns or finds something subtler, like holiday effects."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aiYId1bLLeBr"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "\n",
        "# Create an array 'day_of_week' to store the day of the week for each unique date\n",
        "day_of_week = np.zeros((ndays))\n",
        "\n",
        "# Loop through each unique date\n",
        "for i in range(0, ndays):\n",
        "    # Parse the current date from a string to a datetime object\n",
        "    day_dt = datetime.datetime.strptime(str(days[i]), '%Y%m%d')\n",
        "\n",
        "    # Get the day of the week (1 for Monday, 2 for Tuesday, ..., 7 for Sunday)\n",
        "    day_of_week[i] = day_dt.isoweekday()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UedwK90ZGdZx"
      },
      "outputs": [],
      "source": [
        "# Create a new figure and axis object using subplots\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "# Iterate through each day of the week (from 1 to 7)\n",
        "for i in range(1, 8):\n",
        "    # Find the indices of days that correspond to the current day of the week\n",
        "    day_of_week_index_t = np.where(day_of_week == i)\n",
        "\n",
        "    # Calculate the number of days that match the current day of the week\n",
        "    ndays_t = np.size(day_of_week_index_t[0])\n",
        "\n",
        "    # Plot the average flow for the current day of the week\n",
        "    ax.plot(x_axis_hours,\n",
        "            np.nanmean(vectorized_day_dataset[day_of_week_index_t[0], :].transpose(), 1),\n",
        "            label='day-of-week ' + str(i))\n",
        "    # This line plots the average flow for the current day of the week.\n",
        "    # 'np.nanmean()' calculates the mean while handling NaN values.\n",
        "\n",
        "# Set the y-axis label\n",
        "ax.set_ylabel('Average flow')\n",
        "\n",
        "# Set the x-axis label\n",
        "ax.set_xlabel('5-minute intervals')\n",
        "\n",
        "# Set the x-axis limits to be between 0 and 24\n",
        "plt.xlim(0, 24)\n",
        "\n",
        "# Set the title of the plot\n",
        "ax.set_title('Daily Profile of Flow Dynamics')\n",
        "\n",
        "# Add a legend indicating the day of the week\n",
        "ax.legend()\n",
        "\n",
        "# Display the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TqtvIDWYCWqq"
      },
      "source": [
        "The day's index is from 1 - 7, where 1 is Monday. To our expectations, we can see the difference between weekdays and weekends. So far, we’ve established that traffic days differ in systematic ways: weekday/weekend, time-of-day peaks, occasional anomalies. Clustering will now formalize these differences into groups. The goal is to let the algorithm discover with patterns that we might not have thought to test manually. This we explore with clustering in the next part of the exercise."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Jaw2R9SrfkJ"
      },
      "source": [
        "# Part 2: Clustering\n",
        "\n",
        "In the previous section, we explored our data and prepared it for analysis. Now, we reach the core of our task: clustering. Our goal is to group similar days together to discover common daily traffic patterns, or \"day-types.\"\n",
        "\n",
        "We face a fundamental question in clustering: how many clusters (k) are there? There is no single \"correct\" answer; it depends on the story the data tells and our analytical goals. Once we have our clusters, we will visualize the resulting day-types (cluster centroids) and analyze their characteristics to understand what each \"typical day\" represents ( a weekday rush hour pattern, a quiet weekend pattern, incidents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ON7OnUwzT-ku"
      },
      "source": [
        "## Clustering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hagyMxjjDWmQ"
      },
      "source": [
        "Using clustering methods in scikit-learn is relatively simple, as shown below. With one line of code, you can get some clusters. However, this will need some work to search for representative clusters. This is the first step to pattern exploration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sV_T3yuOr0b9"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
        "from sklearn.mixture import GaussianMixture\n",
        "\n",
        "#clusters = KMeans(n_clusters=10, random_state=0, n_init=\"auto\").fit(vectorized_day_dataset)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zy3KGTezDxo_"
      },
      "source": [
        "As you can see k-means clustering method can not handle missing values, so you have choices: impute data or remove them. We will just remove all days that have missing observations. Fewer days would be removed if we restrict the clustering to a particular day-time period, 06:00 - 22:00:00. Below, we prepare a new dataset without missing values and update the list of days for later visualization purposes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MOfS0A8Xt6mh"
      },
      "outputs": [],
      "source": [
        "n_clusters = 10\n",
        "clusters = None\n",
        "#print(np.where(nans_per_day > 0)[0])\n",
        "vectorized_day_dataset_no_nans = vectorized_day_dataset[np.where(nans_per_day == 0)[0],:]\n",
        "days_not_nans = days[np.where(nans_per_day == 0)[0]]\n",
        "\n",
        "# BELOW lines enables you to comment in and out clustering method you want to use note that GMM have different ouput and thus labels are extracted differently\n",
        "clusters = KMeans(n_clusters=n_clusters, random_state=0, n_init=\"auto\").fit(vectorized_day_dataset_no_nans) # check the parameters at https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\n",
        "# clusters = AgglomerativeClustering(n_clusters=n_clusters,metric='euclidean', linkage='ward').fit(vectorized_day_dataset_no_nans) # check the parameters at https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html\n",
        "# clusters = DBSCAN(eps=500, min_samples = 2).fit(vectorized_day_dataset_no_nans) # check the parameters at https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html\n",
        "\n",
        "if clusters is not None:\n",
        "  cluster_labels = clusters.labels_\n",
        "\n",
        "#cluster_labels = GaussianMixture(n_components=n_clusters).fit(vectorized_day_dataset_no_nans).predict(vectorized_day_dataset_no_nans) #check the parameters at  https://scikit-learn.org/stable/auto_examples/mixture/plot_gmm_init.html#sphx-glr-auto-examples-mixture-plot-gmm-init-py\n",
        "\n",
        "\n",
        "print(cluster_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nv6CseOTz_g"
      },
      "source": [
        "## Visualizaiton of representative day-type patterns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6Dr8JQ0USJk"
      },
      "source": [
        "### Special plots for visualizing day-type patterns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXrQhfpPujQS"
      },
      "source": [
        "the results of clusterings are assignments to the clusters, this can be hard to read and make conclusions about it, so visualization of data in right way is of high importance. Below is a script gives you set of libraries for calendar and centroid visualizaiton.\n",
        "\n",
        "***Note: The below script you do not have to understand. Consider it as an external library that will plot for you, just like a histogram plot, for which you also do not know the exact implementation. Anyway, this course does not focus on information visualization.***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F9A7NgRaul7A"
      },
      "outputs": [],
      "source": [
        "\n",
        "from dateutil.relativedelta import relativedelta\n",
        "from matplotlib.patches import Polygon\n",
        "from matplotlib.lines import Line2D\n",
        "from matplotlib import gridspec\n",
        "from matplotlib.patches import Patch\n",
        "from matplotlib import colors\n",
        "\n",
        "def assign_colors(n_clusters, days, assigments):\n",
        "\n",
        "    days_colors = []\n",
        "    color_to_cluster = []\n",
        "    style_to_cluster = []\n",
        "    weekend_colors = ['#67001f','#d6604d','#fdae61','#f46d43','#d53e4f','#9e0142','#f768a1','#f1c232']#,'#fe9929','#cc4c02','#e31a1c','#737373','#bdbdbd','#252525','#bcbddc']\n",
        "#    weekend_school_colors = ['#c2a5cf','#f1b6da','#8e0152','#c51b7d','#de77ae','#ae017e','#fcc5c0','#e31a1c','#737373','#bdbdbd']\n",
        "#    bank_holidays_colors = ['#543005','#dfc27d','#bf812d','#8c510a']\n",
        "    mixed_colors = ['#4d4d4d','#35978f','#bababa','#878787']\n",
        "    weekday_colors = ['#a6cee3','#1f78b4','#b2df8a','#33a02c','#cab2d6','#6a3d9a','#b15928','#8dd3c7','#bebada','#fb8072','#b3de69','#bc80bd','#fccde5','#ccebc5','#35978f','#80cdc1']\n",
        "\n",
        "    cluster_id_weekdays_share = []\n",
        "    cluster_id_weekend_share = []\n",
        "    cluster_id_all_days = []\n",
        "\n",
        "    for i in range(0,n_clusters):\n",
        "        color_to_cluster.append(None)\n",
        "        style_to_cluster.append(None)\n",
        "        cluster_id_weekdays_share.append(0)\n",
        "        cluster_id_weekend_share.append(0)\n",
        "        cluster_id_all_days.append(0)\n",
        "\n",
        "    for i in range(0,len(days)):\n",
        "        #print(i,assigments[i],len(assigments),len(cluster_id_all_days))\n",
        "        if assigments[i] is not None:\n",
        "            cluster_id_all_days[assigments[i]] += 1\n",
        "            if '-' in str(days[i]):\n",
        "                pomT = datetime.datetime.strptime(str(days[i]),'%Y-%m-%d')\n",
        "            else:\n",
        "                pomT = datetime.datetime.strptime(str(days[i]),'%Y%m%d')\n",
        "\n",
        "            if int(pomT.weekday()) < 5:\n",
        "                cluster_id_weekdays_share[assigments[i]] += 1\n",
        "            else:\n",
        "                cluster_id_weekend_share[assigments[i]] += 1\n",
        "\n",
        "    print('cluster_id_weekdays_share',cluster_id_weekdays_share)\n",
        "    print('cluster_id_weekend_share',cluster_id_weekend_share)\n",
        "    for i in range(0,len(days)):\n",
        "        if assigments[i] is not None:\n",
        "            cluster_idx = assigments[i]\n",
        "            if '-' in str(days[i]):\n",
        "                pomT = datetime.datetime.strptime(str(days[i]),'%Y-%m-%d')\n",
        "            else:\n",
        "                pomT = datetime.datetime.strptime(str(days[i]),'%Y%m%d')\n",
        "            if color_to_cluster[assigments[i]] is None:\n",
        "                if cluster_id_weekend_share[cluster_idx] / float(cluster_id_all_days[cluster_idx]) > 0.6:\n",
        "                        color_to_cluster[assigments[i]] = weekend_colors.pop()\n",
        "                        style_to_cluster[assigments[i]] = ':'\n",
        "                elif cluster_id_weekdays_share[cluster_idx] / float(cluster_id_all_days[cluster_idx]) > 0.6:\n",
        "                        color_to_cluster[assigments[i]] = weekday_colors.pop(0)\n",
        "                        style_to_cluster[assigments[i]] = '-'\n",
        "                else:\n",
        "                    color_to_cluster[assigments[i]] = mixed_colors.pop()\n",
        "                    style_to_cluster[assigments[i]] = ':'\n",
        "\n",
        "            days_colors.append(color_to_cluster[assigments[i]])\n",
        "        else:\n",
        "            days_colors.append(None)\n",
        "\n",
        "    return days_colors,color_to_cluster,style_to_cluster\n",
        "\n",
        "\n",
        "def calmap(ax, year, data, days, assigments, n_clusters,days_colors,color_to_cluster,\n",
        "           limit_graphics=False):\n",
        "\n",
        "    ax.tick_params('x', length=0, labelsize=\"medium\", which='major')\n",
        "    ax.tick_params('y', length=0, labelsize=\"x-small\", which='major')\n",
        "\n",
        "    # Month borders\n",
        "\n",
        "    xticks, labels = [], []\n",
        "    start = datetime.datetime(year,1,1).weekday()\n",
        "\n",
        "    for month in range(1,13):\n",
        "\n",
        "        first = datetime.datetime(year, month, 1)\n",
        "        last = first + relativedelta(months=1, days=-1)\n",
        "\n",
        "        y0 = first.weekday()\n",
        "        y1 = last.weekday()\n",
        "        x0 = (int(first.strftime(\"%j\"))+start-1)//7\n",
        "        x1 = (int(last.strftime(\"%j\"))+start-1)//7\n",
        "\n",
        "        P = [ (x0,   y0), (x0,    7),  (x1,   7),\n",
        "              (x1,   y1+1), (x1+1,  y1+1), (x1+1, 0),\n",
        "              (x0+1,  0), (x0+1,  y0) ]\n",
        "\n",
        "        xticks.append(x0 +(x1-x0+1)/2)\n",
        "        labels.append(first.strftime(\"%b\"))\n",
        "        poly = Polygon(P, edgecolor=\"black\", facecolor=\"None\",\n",
        "\n",
        "                       linewidth=1, zorder=20, clip_on=False)\n",
        "\n",
        "        ax.add_artist(poly)\n",
        "\n",
        "    line = Line2D([0,53],[5,5],linewidth=1, zorder = 20,color=\"black\",linestyle='dashed')\n",
        "    ax.add_artist(line)\n",
        "\n",
        "    if not limit_graphics:\n",
        "        ax.set_xticks(xticks)\n",
        "        ax.set_xticklabels(labels)\n",
        "        ax.set_yticks(0.5 + np.arange(7))\n",
        "        ax.set_yticklabels([\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"])\n",
        "        ax.set_title(\"{}\".format(year), weight=\"semibold\")\n",
        "    else:\n",
        "        plt.tick_params(\n",
        "            axis='x',          # changes apply to the x-axis\n",
        "            which='both',      # both major and minor ticks are affected\n",
        "            bottom=False,      # ticks along the bottom edge are off\n",
        "            top=False,         # ticks along the top edge are off\n",
        "            labelbottom=False)\n",
        "        plt.tick_params(\n",
        "            axis='y',          # changes apply to the x-axis\n",
        "            which='both',      # both major and minor ticks are affected\n",
        "            left=False,      # ticks along the bottom edge are off\n",
        "            right=False,         # ticks along the top edge are off\n",
        "            labelleft=False)\n",
        "\n",
        "    # Clearing first and last day from the data\n",
        "    valid = datetime.datetime(year, 1, 1).weekday()\n",
        "    data[:valid,0] = np.nan\n",
        "    valid = datetime.datetime(year, 12, 31).weekday()\n",
        "    # data[:,x1+1:] = np.nan\n",
        "    data[valid+1:,x1] = np.nan\n",
        "\n",
        "    for i in range(0,len(days)):\n",
        "        if '-' in str(days[i]):\n",
        "            pomT = datetime.datetime.strptime(str(days[i]),'%Y-%m-%d')\n",
        "        else:\n",
        "            pomT = datetime.datetime.strptime(str(days[i]),'%Y%m%d')\n",
        "        week_number = int(pomT.strftime(\"%W\"))\n",
        "        day_of_week = int(pomT.weekday())\n",
        "        data[day_of_week,week_number] = assigments[i]\n",
        "\n",
        "\n",
        "    act_date = datetime.datetime(year,1,1)\n",
        "    while (act_date.year == year):\n",
        "\n",
        "        week_number = int(act_date.strftime(\"%W\"))\n",
        "        day_of_week = int(act_date.weekday())\n",
        "        doy_id = act_date.timetuple().tm_yday\n",
        "        if doy_id<5 and week_number > 53:\n",
        "            week_number = 0\n",
        "\n",
        "        act_date = act_date + datetime.timedelta(days=1)\n",
        "\n",
        "    #pomT = datetime.datetime.strptime('2017-01-01','%Y-%m-%d')\n",
        "    #week_number = int(pomT.strftime(\"%V\"))\n",
        "    #day_of_week = int(pomT.weekday())\n",
        "    #print(week_number,day_of_week)\n",
        "    #doy_id = pomT.timetuple().tm_yday\n",
        "    #if doy_id<5 and week_number > 0:\n",
        "    #    week_number = 0\n",
        "    #data[day_of_week,week_number] = len(clusters)+10\n",
        "\n",
        "    # Showing data\n",
        "    cmap = plt.cm.spring  # Can be any colormap that you want after the cm\n",
        "    cmap.set_bad(color='white')\n",
        "\n",
        "    #ax.imshow(data, extent=[0,53,0,7], zorder=10, vmin=0, vmax=len(clusters)+10,\n",
        "    #          cmap=cmap, origin=\"lower\", alpha=.75)\n",
        "\n",
        "    cmap = colors.ListedColormap(color_to_cluster)\n",
        "    bounds=[-0.1]\n",
        "    step = 1\n",
        "    for i in range(0,n_clusters):\n",
        "        bounds.append(i-0.1+step)\n",
        "    norm = colors.BoundaryNorm(bounds, cmap.N)\n",
        "    #print(color_to_cluster)\n",
        "   #print(bounds)\n",
        "    #print(norm)\n",
        "\n",
        "    #print(color_to_cluster)\n",
        "    #print(bounds)\n",
        "    #print(cmap)\n",
        "    #exit(0)\n",
        "\n",
        "    ax.imshow(data, extent=[0,53,0,7], zorder=10, interpolation='nearest', origin='lower',cmap=cmap, norm=norm)\n",
        "\n",
        "def make_calendar_visualization_figure(days,assigments,n_clusters,years,days_colors,color_to_cluster,\n",
        "                                       save_figure: str = None, show_figure:bool = True, limit_graphics = False):\n",
        "\n",
        "    fig = plt.figure(figsize=(8,1.5*len(years)), dpi=100)\n",
        "    X = np.linspace(-1,1, 53*7)\n",
        "\n",
        "    for i, obj in enumerate(years):\n",
        "\n",
        "        pom_s = str(len(years))+'1'+str(i+1)\n",
        "        print(pom_s)\n",
        "\n",
        "        ax = plt.subplot(int(pom_s), xlim=[0, 53], ylim=[0, 7], frameon=False, aspect=1)\n",
        "        I = 1.2 - np.cos(X.ravel()) + np.random.normal(0,.2, X.size)\n",
        "        I = I.reshape(53,7).T\n",
        "        I.fill(np.nan)\n",
        "        calmap(ax, int(obj), I.reshape(53,7).T, days, assigments, n_clusters,days_colors,color_to_cluster, limit_graphics)\n",
        "\n",
        "    #   ax = plt.subplot(212, xlim=[0,53], ylim=[0,7], frameon=False, aspect=1)\n",
        "    #  I = 1.1 - np.cos(X.ravel()) + np.random.normal(0,.2, X.size)\n",
        "    #   calmap(ax, 2018, I.reshape(53,7).T)\n",
        "\n",
        "    #ax = plt.subplot(313, xlim=[0,53], ylim=[0,7], frameon=False, aspect=1)\n",
        "    #I = 1.0 - np.cos(X.ravel()) + np.random.normal(0,.2, X.size)\n",
        "    #calmap(ax, 2019, I.reshape(53,7).T)\n",
        "    if save_figure:\n",
        "        plt.savefig(save_figure)\n",
        "\n",
        "    if show_figure or save_figure is None:\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "def make_figure_centroids(x,y,color_to_cluster,style_to_cluster,cluster_ids,minY = None,maxY = None,\n",
        "                          save_figure: str = None, show_figure:bool = True):\n",
        "\n",
        "    #print(color_to_cluster)\n",
        "    fig = plt.figure(figsize=(8,3))\n",
        "    ax = fig.add_subplot(111)\n",
        "    for i in range(0,len(x)):\n",
        "        #print(i,color_to_cluster[i],style_to_cluster[i])\n",
        "        #print(y[i])\n",
        "        ax.plot(x[i],y[i],style_to_cluster[i], color=color_to_cluster[i], label=str(cluster_ids[i]))\n",
        "    ax.set_xlabel('Time of day')\n",
        "    ax.set_ylabel('Flow')\n",
        "    if minY is not None and maxY is not None:\n",
        "        ax.set_ylim([minY, maxY])\n",
        "    plt.legend()\n",
        "\n",
        "    if save_figure:\n",
        "        plt.savefig(save_figure)\n",
        "\n",
        "    if show_figure or save_figure is None:\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0HZj5jpUf6n"
      },
      "source": [
        "### Visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOm8M7jPUuCl"
      },
      "source": [
        "#### Calendar visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3rx31eUEuYb"
      },
      "source": [
        "Using the above functions for visualization, the representative day-type clusters can be seen in a calendar form in order to enable us by-eye analysis to make sense of these clusters. Do cluster labels form temporally coherent patterns when viewed across a calendar? In particular, do weekends, holidays or seasonal cycles explain cluster placement? Note that white cells are removed days because of missing observations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PPwbOdp1voEh"
      },
      "outputs": [],
      "source": [
        "# Calculate the number of clusters by finding unique values in 'cluster_labels'\n",
        "n_clusters_t = len(np.unique(cluster_labels))\n",
        "\n",
        "# Assign colors to days based on clusters\n",
        "days_colors, color_to_cluster, style_to_cluster = assign_colors(n_clusters_t, days_not_nans, cluster_labels)\n",
        "# The function 'assign_colors' is used to determine colors and styles for visualization.\n",
        "\n",
        "# Create a calendar visualization figure\n",
        "make_calendar_visualization_figure(days_not_nans, cluster_labels, n_clusters_t, [2021], days_colors,\n",
        "                                   color_to_cluster, save_figure=None)\n",
        "# This function 'make_calendar_visualization_figure' is used to generate a visualization based on the provided data and parameters.\n",
        "# 'days_not_nans' are the days, 'cluster_labels' are the cluster labels, 'n_clusters_t' is the number of clusters,\n",
        "# '[2021]' represents the year, 'days_colors' represent the assigned colors for each day, 'color_to_cluster' maps colors to clusters,\n",
        "# and 'save_figure' is an optional parameter to save the generated figure (can be None if not saving)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3V4Ln1QSU0fZ"
      },
      "source": [
        "#### Day-time profile of centroids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IeUR_UqR4e1A"
      },
      "outputs": [],
      "source": [
        "# Initialize empty lists to store centroid data\n",
        "centroids_xx = []  # x-axis values for centroids\n",
        "centroids_yy_daytypes = []  # y-axis values for centroids, grouped by day types\n",
        "cluster_ids = []  # Cluster IDs\n",
        "\n",
        "# Iterate through each cluster\n",
        "for i in range(0, n_clusters_t):\n",
        "    # Store the x-axis values for centroids (hours of the day)\n",
        "    centroids_xx.append(x_axis_hours)\n",
        "\n",
        "    # Calculate the y-axis values for centroids (average flow for each 5-minute interval)\n",
        "    centroid_yy = list(np.nanmean(vectorized_day_dataset_no_nans[np.where(cluster_labels == i)[0], :], 0).transpose())\n",
        "    centroids_yy_daytypes.append(centroid_yy)\n",
        "\n",
        "    # Store the cluster ID\n",
        "    cluster_ids.append(i)\n",
        "\n",
        "# Generate a figure displaying the centroids\n",
        "make_figure_centroids(centroids_xx, centroids_yy_daytypes, color_to_cluster, style_to_cluster, cluster_ids)\n",
        "# The function 'make_figure_centroids' is used to create a visualization of the centroids,\n",
        "# with the provided data and parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8ocQl6CGIj6"
      },
      "source": [
        "Next, we can explore the day-type profiles. Centroids compress intra-day variability into an average, smoothness and within-cluster spread must be inspected together. A sharp, narrow peak in a centroid with small within-cluster spread implies a reliable, repeatable event. A noisy centroid suggests heterogeneity or too few members."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1_7BaeDGOOn"
      },
      "source": [
        "As you can see with first shot using k-means we can see much more day-type patterns as we may expect, it even enable to identify potential outliers/incident like cluster 7."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bh806WebYlyQ"
      },
      "source": [
        "# Part 3: Independent work - clustering evaluation\n",
        "\n",
        "Find the day-type clusterings that you consider most representative. Evaluate clustering quality using internal evaluation metrics such as Silhouette score and others. You can consider short-term prediction as an external metric that imitates how useful recognized patterns are compared to newly observed days. In an assignment, motivate why you consider the method selected by you as superior.\n",
        "\n",
        "**NOTE** ***This part of exercise as well with all you have learned about clustering is the basis for your reflection in mandatory grated assignment for the MODULE 5***\n",
        "\n",
        "You can use above and below scripts for experimenting with different methods, metrics and number of clusters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ReJNTPK1ZQ2U"
      },
      "source": [
        "## Internal evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3SQZh9zZmpp"
      },
      "source": [
        "Are clusters internally compact and well separated under the chosen metric? Below is an example for using internal metrics in scikit-learn."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_eAXdj_bZVAG"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Import relevant metrics from scikit-learn\n",
        "from sklearn.metrics import calinski_harabasz_score, silhouette_score, davies_bouldin_score\n",
        "\n",
        "# Calculate the Silhouette Score\n",
        "SC_score = silhouette_score(vectorized_day_dataset_no_nans, cluster_labels)\n",
        "# Silhouette Score measures the quality of clusters, higher values indicate better separation.\n",
        "\n",
        "# Calculate the Davies-Bouldin Score\n",
        "DB_score = davies_bouldin_score(vectorized_day_dataset_no_nans, cluster_labels)\n",
        "# Davies-Bouldin Score measures the average similarity between each cluster and its most similar cluster, lower values indicate better separation.\n",
        "\n",
        "# Calculate the Calinski-Harabasz Score\n",
        "CH_score = calinski_harabasz_score(vectorized_day_dataset_no_nans, cluster_labels)\n",
        "# Calinski-Harabasz Score measures the ratio of between-cluster variance to within-cluster variance, higher values indicate better separation.\n",
        "\n",
        "# Print the computed cluster quality scores\n",
        "print('Silhouette Score:', SC_score)\n",
        "print('Davies-Bouldin Score:', DB_score)\n",
        "print('Calinski-Harabasz Score:', CH_score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cVhO7--eXHGa"
      },
      "source": [
        "Internal metrics help tune the number of clusters and preprocessing. They are necessary but not sufficient for usefulness: good silhouette does not guarantee better prediction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtggY3IXZVYO"
      },
      "source": [
        "## External evaluation with short-term prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8GM5qTBZZgt"
      },
      "source": [
        "So far, we have evaluated our clusters internally using metrics like silhouette score. But how do we know if these discovered \"day-types\" are actually useful for a real-world task? Do centroids improve one-step-ahead forecasting relative to meaningful baselines?\n",
        "\n",
        "The idea is simple: if our centroids truly capture meaningful patterns, they should provide a good \"baseline\" prediction for what traffic will look like in the very near future. We will test this by using the centroids to predict the next 5-minute interval, a common task in traffic management systems.\n",
        "\n",
        "First, lets load the evaluation dataset used of evaluating short-term prediction accuracy, vectorize it to day vectors and remove missing values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-1dqEbTgZfnx"
      },
      "outputs": [],
      "source": [
        "# Read the evaluation dataset from a CSV file\n",
        "data_eval_df = pd.read_csv(\"evaluation_dataset_exercise_5_clustering_highway_traffic.csv\", sep=\";\")\n",
        "\n",
        "# Sort the evaluation DataFrame by columns \"Date\" and \"Interval_5\"\n",
        "data_eval_df.sort_values([\"Date\", \"Interval_5\"])\n",
        "\n",
        "# Extract unique dates from the sorted evaluation DataFrame\n",
        "days_eval = np.unique(data_eval_df[['Date']].values.ravel())\n",
        "# Calculate the total number of unique days in the evaluation dataset\n",
        "ndays_eval = len(days_eval)\n",
        "\n",
        "# Group the evaluation DataFrame by the \"Date\" column\n",
        "day_eval_subsets_df = data_eval_df.groupby([\"Date\"])\n",
        "\n",
        "# Initialize a matrix 'vectorized_day_dataset_eval' filled with NaN values\n",
        "vectorized_day_dataset_eval = np.zeros((ndays_eval, nintvals))\n",
        "vectorized_day_dataset_eval.fill(np.nan)\n",
        "# This section initializes a 2D array to store the evaluation dataset and fills it with NaN values.\n",
        "\n",
        "# Loop through each unique day in the evaluation dataset\n",
        "for i in range(0, ndays_eval):\n",
        "    # Get the DataFrame corresponding to the current day\n",
        "    df_t = day_eval_subsets_df.get_group(days_eval[i])\n",
        "\n",
        "    # Loop through each row in the current day's DataFrame\n",
        "    for j in range(len(df_t)):\n",
        "        # Get the current day's DataFrame (this line is redundant)\n",
        "        df_t = day_eval_subsets_df.get_group(days_eval[i])\n",
        "\n",
        "        # Extract the \"Interval_5\" and \"flow\" values and populate 'vectorized_day_dataset_eval'\n",
        "        vectorized_day_dataset_eval[i, df_t.iloc[j][\"Interval_5\"]] = df_t.iloc[j][\"flow\"]\n",
        "\n",
        "# Print the resulting 'vectorized_day_dataset_eval'\n",
        "print(vectorized_day_dataset_eval)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nNUDJ57Uj7JZ"
      },
      "outputs": [],
      "source": [
        "# Calculate the total number of NaN values in the evaluation dataset\n",
        "print('Number of NaNs:', np.sum(np.isnan(vectorized_day_dataset_eval)))\n",
        "\n",
        "# Calculate the rate of NaN values in the evaluation dataset\n",
        "print('Rate of NaNs:', np.sum(np.isnan(vectorized_day_dataset_eval)) / (ndays_eval * nintvals))\n",
        "\n",
        "# Calculate the number of days with missing values\n",
        "nans_per_day_eval = np.sum(np.isnan(vectorized_day_dataset_eval), 1)\n",
        "print('Number of days with missing values:', np.size(np.where(nans_per_day_eval > 0)))\n",
        "\n",
        "# Filter out days with no missing values and create a new dataset\n",
        "vectorized_day_dataset_no_nans_eval = vectorized_day_dataset_eval[np.where(nans_per_day_eval == 0)[0], :]\n",
        "days_not_nans_eval = days_eval[np.where(nans_per_day_eval == 0)[0]]\n",
        "\n",
        "# Calculate the final number of days in the evaluation dataset after removing missing values\n",
        "print('Final number of days in evaluation dataset:', len(days_not_nans_eval))\n",
        "\n",
        "# Print the list of days in the evaluation dataset with no missing values\n",
        "print('List of days without missing values:', days_not_nans_eval)\n",
        "\n",
        "# Calculate the total number of days in the filtered evaluation dataset\n",
        "ndays_eval_not_nans = len(days_not_nans_eval)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_Yc2vg9gUrJ"
      },
      "source": [
        "Now when the dataset for evaluation is ready, below is a script for evaluating short-term prediction performance.\n",
        "\n",
        "We will implement a specific forecasting strategy:\n",
        "\n",
        "For each 5-minute interval j in a new day:\n",
        "\n",
        "1. Look at the recent past: We take the actual observed traffic flow from the last 5 intervals (the last 25 minutes: j-4 to j).\n",
        "\n",
        "2. Find the best-matching pattern: We compare this short recent history against the beginning of each of our four cluster centroids. The centroid whose first j+1 intervals most closely match the recent past is selected as our \"best guide\" for what should happen next.\n",
        "\n",
        "3. Make a prediction: We use the value of the best-matching centroid at the next time interval (j+1) as our prediction.\n",
        "\n",
        "4. Measure error: We compare this prediction to the actual observed traffic flow at interval j+1 and calculate the error.\n",
        "\n",
        "We repeat this process for every possible interval in every day in our evaluation set. This tests the centroids' predictive power in a real context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6259sc-sgVE8"
      },
      "outputs": [],
      "source": [
        "# Import the pairwise_distances function from scikit-learn's metrics library\n",
        "import sklearn.metrics.pairwise as dis_lib\n",
        "\n",
        "# Define a function to find the closest centroid to a new data point within a specified day-time interval range\n",
        "def find_the_closest_centroid(centroids, new_day, from_interval: int, to_interval: int):\n",
        "    closest_centroid = None\n",
        "    closest_dist = None\n",
        "\n",
        "    # Iterate through each centroid\n",
        "    for i in range(0, len(centroids)):\n",
        "        # Calculate the Euclidean distance between the centroid and the new data point\n",
        "        ed_t = dis_lib.paired_distances(centroids[i], new_day, metric='euclidean')\n",
        "\n",
        "        # Check if the current centroid is closer than the previously closest one\n",
        "        if closest_centroid is None or closest_dist > ed_t:\n",
        "            closest_centroid = i\n",
        "            closest_dist = ed_t\n",
        "\n",
        "    return closest_centroid\n",
        "\n",
        "# Initialize a list to store centroid data\n",
        "centroids = []\n",
        "\n",
        "# Calculate centroids for each cluster\n",
        "for i in np.unique(cluster_labels):\n",
        "    centroid = np.nanmean(vectorized_day_dataset_no_nans[np.where(cluster_labels == i)[0], :], 0).reshape(1, nintvals)\n",
        "    centroids.append(centroid)\n",
        "\n",
        "# Define the number of past intervals to consider for classification\n",
        "n_past_intervals_for_classification = 5\n",
        "\n",
        "# Initialize variables to calculate accuracy metrics\n",
        "total_mae = 0\n",
        "total_mape = 0\n",
        "prediction_counts = 0\n",
        "\n",
        "# Loop through each day in the evaluation dataset with no missing values\n",
        "for i in range(0, ndays_eval_not_nans):\n",
        "    # Loop through intervals from n_past_intervals_for_classification to nintvals - 1\n",
        "    for j in range(n_past_intervals_for_classification, nintvals - 1):\n",
        "        # Find the closest centroid for the current data point\n",
        "        centroid_index = find_the_closest_centroid(centroids, vectorized_day_dataset_no_nans_eval[i].reshape(1, nintvals), j - n_past_intervals_for_classification, j)\n",
        "\n",
        "        # Predict the value for the next interval\n",
        "        predicted_value = centroids[centroid_index][0, j + 1]\n",
        "\n",
        "        # Calculate Mean Absolute Error (MAE) and Mean Absolute Percentage Error (MAPE)\n",
        "        mae_t = abs(predicted_value - vectorized_day_dataset_no_nans_eval[i][j + 1])\n",
        "        mape_t = abs(predicted_value - vectorized_day_dataset_no_nans_eval[i][j + 1]) / float(vectorized_day_dataset_no_nans_eval[i][j + 1])\n",
        "\n",
        "        # Accumulate MAE, MAPE, and count of predictions\n",
        "        total_mae += mae_t\n",
        "        total_mape += mape_t\n",
        "        prediction_counts += 1\n",
        "\n",
        "# Calculate and print the prediction accuracy metrics\n",
        "print('Prediction accuracy MAE:', total_mae / prediction_counts)\n",
        "print('Prediction accuracy MAPE:', total_mape / prediction_counts)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xl7MMWdXHGa"
      },
      "source": [
        "External evaluation quantifies the performance of the day types with their centroids, which provides generalized data model of the dataset. Furthermore, it enables the comparison of various clusterings. The better the prediction performance, the more effectively the cluster-based prediction model captures important representative patterns, and also allows for better classification of unseen partial days. This score is closer to the objective of finding representative day-types, as are the internal evaluation scores, because it measures the predictive power of recognized representative patterns, but also the classification of unseen days (by Euclidean distance in this case)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (Spyder)",
      "language": "python3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}